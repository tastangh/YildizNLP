2024-10-31 21:10:18,060 - INFO - Reading CSV file...
2024-10-31 21:10:18,074 - INFO - CSV file successfully read.
2024-10-31 21:10:18,962 - INFO - Using device: cuda
2024-10-31 21:10:18,964 - INFO - Processing model: jinaai/jina-embeddings-v3
2024-10-31 21:10:18,994 - INFO - Reading CSV file...
2024-10-31 21:10:19,007 - INFO - CSV file successfully read.
2024-10-31 21:10:19,970 - INFO - Using device: cuda
2024-10-31 21:10:19,973 - INFO - Processing model: sentence-transformers/all-MiniLM-L12-v2
2024-10-31 21:10:24,581 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,585 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,593 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,602 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,612 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,619 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,957 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,962 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,968 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,975 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,983 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,989 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:24,995 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,004 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,011 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,017 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,024 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,029 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,035 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,041 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,047 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,054 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,060 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,069 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,076 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:25,142 - INFO - Starting epoch 1/1 for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:26,045 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:28,357 - INFO - Starting epoch 1/1 for jinaai/jina-embeddings-v3...
2024-10-31 21:10:29,189 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:10:30,488 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:31,435 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,437 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,441 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,445 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,450 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,455 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,462 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,466 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,468 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,473 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,476 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,479 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,482 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,485 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,488 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,491 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,494 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,498 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,501 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,504 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,507 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,508 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,513 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,516 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:31,519 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:32,433 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:10:32,509 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 1: Training Top 1 Accuracy: 38.00%
2024-10-31 21:10:32,510 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 1: Training Top 5 Accuracy: 51.33%
2024-10-31 21:10:33,520 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:35,532 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:10:36,445 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:10:37,497 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:41,692 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:10:41,777 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 2: Training Top 1 Accuracy: 40.00%
2024-10-31 21:10:41,778 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 2: Training Top 5 Accuracy: 54.00%
2024-10-31 21:10:42,864 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:46,787 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:10:47,001 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:10:48,194 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:50,170 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,171 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,175 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,178 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,180 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,181 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,183 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,187 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,189 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,191 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,193 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,196 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,198 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,200 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,203 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,205 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,208 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,210 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,213 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,214 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,217 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,219 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,221 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,223 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,225 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:10:50,949 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:10:51,006 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 3: Training Top 1 Accuracy: 40.00%
2024-10-31 21:10:51,006 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 3: Training Top 5 Accuracy: 56.67%
2024-10-31 21:10:53,144 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:10:54,805 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:10:57,510 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:10:58,776 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:03,897 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:03,931 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 4: Training Top 1 Accuracy: 34.00%
2024-10-31 21:11:03,932 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 4: Training Top 5 Accuracy: 50.00%
2024-10-31 21:11:04,929 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:05,702 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:11:06,614 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 1: Training Top 1 Accuracy: 93.33%
2024-10-31 21:11:06,615 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 1: Training Top 5 Accuracy: 97.33%
2024-10-31 21:11:07,261 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:08,636 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:09,798 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,800 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,804 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,807 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,810 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,813 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,817 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,821 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,824 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,827 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,831 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,834 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,836 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,841 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,844 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,846 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,849 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,851 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,854 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,859 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,864 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,866 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,872 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,878 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:09,883 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:12,783 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:12,855 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 5: Training Top 1 Accuracy: 33.33%
2024-10-31 21:11:12,856 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 5: Training Top 5 Accuracy: 51.33%
2024-10-31 21:11:14,433 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:15,870 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:16,675 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:11:17,330 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:18,856 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:18,878 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 6: Training Top 1 Accuracy: 50.00%
2024-10-31 21:11:18,878 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Epoch 1, Batch 6: Training Top 5 Accuracy: 66.00%
2024-10-31 21:11:20,021 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:21,829 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:22,657 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:24,597 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:24,642 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Validation Top 1 Accuracy: 41.00%
2024-10-31 21:11:24,642 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Validation Top 5 Accuracy: 55.00%
2024-10-31 21:11:25,703 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:28,096 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:28,437 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:11:29,021 - INFO - Getting embeddings for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:31,058 - INFO - Embeddings for sentence-transformers/all-MiniLM-L12-v2 obtained.
2024-10-31 21:11:31,097 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Test Top 1 Accuracy: 40.00%
2024-10-31 21:11:31,097 - INFO - sentence-transformers/all-MiniLM-L12-v2 - Test Top 5 Accuracy: 56.00%
2024-10-31 21:11:31,099 - INFO - Applying TSNE for sentence-transformers/all-MiniLM-L12-v2...
2024-10-31 21:11:32,302 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,303 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,305 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,308 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,311 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,313 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,316 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,319 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,322 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,325 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,327 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,331 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,334 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,336 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,339 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,342 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,345 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,348 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,351 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,354 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,358 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,361 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,364 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,367 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:32,370 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:11:37,159 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:11:58,814 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:11:59,666 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 2: Training Top 1 Accuracy: 93.33%
2024-10-31 21:11:59,666 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 2: Training Top 5 Accuracy: 98.00%
2024-10-31 21:12:11,548 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,549 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,551 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,554 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,556 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,558 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,560 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,564 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,571 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,575 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,579 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,582 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,585 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,588 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,590 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,594 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,596 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,598 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,600 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,602 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,605 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,607 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,610 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,612 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:11,614 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:22,875 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:12:36,880 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:12:39,626 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,627 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,629 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,631 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,634 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,635 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,637 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,640 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,642 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,644 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,646 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,648 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,650 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,652 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,654 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,656 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,658 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,659 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,661 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,663 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,665 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,667 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,670 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,672 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:39,674 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:44,258 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:12:56,672 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:12:57,417 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 3: Training Top 1 Accuracy: 95.33%
2024-10-31 21:12:57,417 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 3: Training Top 5 Accuracy: 98.67%
2024-10-31 21:12:59,293 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,294 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,297 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,300 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,303 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,306 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,309 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,311 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,314 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,316 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,319 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,320 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,323 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,325 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,327 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,330 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,333 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,335 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,337 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,339 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,341 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,343 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,345 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,347 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:12:59,349 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:02,080 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:13:15,233 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:13:17,738 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,740 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,746 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,750 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,754 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,758 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,761 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,765 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,768 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,771 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,773 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,776 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,778 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,780 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,781 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,783 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,785 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,786 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,788 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,790 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,792 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,793 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,795 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,796 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:17,799 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:21,114 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:13:33,268 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:13:34,078 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 4: Training Top 1 Accuracy: 96.00%
2024-10-31 21:13:34,079 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 4: Training Top 5 Accuracy: 98.67%
2024-10-31 21:13:36,332 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,332 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,334 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,336 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,338 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,340 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,342 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,345 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,347 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,349 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,351 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,354 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,355 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,357 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,359 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,361 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,363 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,366 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,368 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,370 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,373 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,376 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,378 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,380 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:36,383 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:39,673 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:13:47,173 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:13:49,355 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,355 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,358 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,362 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,365 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,367 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,370 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,374 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,376 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,379 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,380 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,383 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,385 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,388 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,390 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,392 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,394 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,396 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,399 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,402 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,404 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,406 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,407 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,409 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:49,411 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:13:51,993 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:13:59,624 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:14:00,196 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 5: Training Top 1 Accuracy: 93.33%
2024-10-31 21:14:00,197 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 5: Training Top 5 Accuracy: 98.67%
2024-10-31 21:14:01,811 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,812 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,814 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,816 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,820 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,826 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,831 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,837 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,840 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,844 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,847 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,850 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,852 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,855 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,857 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,858 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,860 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,862 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,864 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,865 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,868 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,869 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,871 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,873 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:01,876 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:04,177 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:14:06,615 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:14:11,102 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,103 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,106 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,109 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,111 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,122 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,128 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,132 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,136 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,141 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,145 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,148 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,150 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,154 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,156 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,158 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,160 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,162 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,163 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,165 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,167 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,170 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,172 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,174 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:11,176 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:13,680 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:14:16,009 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:14:16,530 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 6: Training Top 1 Accuracy: 98.00%
2024-10-31 21:14:16,530 - INFO - jinaai/jina-embeddings-v3 - Epoch 1, Batch 6: Training Top 5 Accuracy: 98.00%
2024-10-31 21:14:18,832 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,834 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,839 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,842 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,845 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,848 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,850 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,853 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,855 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,859 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,862 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,867 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,871 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,874 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,877 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,879 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,881 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,883 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,886 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,888 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,890 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,892 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,894 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,895 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:18,897 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:20,921 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:14:25,410 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:14:28,278 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,279 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,283 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,285 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,288 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,291 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,295 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,300 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,305 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,309 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,313 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,316 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,318 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,321 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,324 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,325 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,327 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,329 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,333 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,335 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,337 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,339 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,341 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,343 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:28,346 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:30,392 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:14:34,842 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:14:35,376 - INFO - jinaai/jina-embeddings-v3 - Validation Top 1 Accuracy: 94.00%
2024-10-31 21:14:35,376 - INFO - jinaai/jina-embeddings-v3 - Validation Top 5 Accuracy: 98.00%
2024-10-31 21:14:37,616 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,616 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,621 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,623 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,626 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,628 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,630 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,632 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,635 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,637 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,639 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,641 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,643 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,645 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,647 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,650 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,651 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,653 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,656 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,658 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,659 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,661 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,663 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,665 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:37,669 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:39,710 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:14:44,308 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:14:47,249 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,250 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,254 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,257 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,260 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,264 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,266 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,269 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,274 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,280 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,283 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,285 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,287 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,290 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,293 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,296 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,299 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,303 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,306 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,308 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,311 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,313 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,316 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,318 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:47,320 - WARNING - flash_attn is not installed. Using PyTorch native attention implementation.
2024-10-31 21:14:49,249 - INFO - Getting embeddings for jinaai/jina-embeddings-v3...
2024-10-31 21:14:54,402 - INFO - Embeddings for jinaai/jina-embeddings-v3 obtained.
2024-10-31 21:14:54,903 - INFO - jinaai/jina-embeddings-v3 - Test Top 1 Accuracy: 95.00%
2024-10-31 21:14:54,903 - INFO - jinaai/jina-embeddings-v3 - Test Top 5 Accuracy: 99.00%
2024-10-31 21:14:54,904 - INFO - Applying TSNE for jinaai/jina-embeddings-v3...
2024-10-31 21:14:59,750 - INFO - TSNE visualization completed for jinaai/jina-embeddings-v3.
